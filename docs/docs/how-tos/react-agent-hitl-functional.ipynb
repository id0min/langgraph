{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to add human-in-the-loop processes to a ReAct agent (Functional API)\n",
    "\n",
    "!!! info \"Prerequisites\"\n",
    "    This guide assumes familiarity with the following:\n",
    "\n",
    "    - Implementing [human-in-the-loop](../../concepts/human_in_the_loop) workflows with [interrupt](../../concepts/human_in_the_loop/#interrupt)\n",
    "    - [How to create a ReAct agent using the Functional API](react-agent-from-scratch-functional.ipynb)\n",
    "\n",
    "This guide demonstrates how to implement human-in-the-loop workflows in a ReAct agent using the LangGraph [Functional API](../../concepts/functional_api).\n",
    "\n",
    "We will build off of the agent created in the [How to create a ReAct agent using the Functional API](react-agent-from-scratch-functional.ipynb) guide.\n",
    "\n",
    "Specifically, we will demonstrate:\n",
    "\n",
    "1. [How to allow the model to reach out to a human for assistance](#reach-out-to-a-human-for-assistance)\n",
    "2. [How to review tool calls](#review-tool-calls-before-execution)\n",
    "\n",
    "Both of these goals can be accomplished through use of the [interrupt](../../concepts/human_in_the_loop/#interrupt) function at key points in our application.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install the required packages and set our API keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "     <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for better debugging</p>\n",
    "     <p style=\"padding-top: 5px;\">\n",
    "         Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM aps built with LangGraph â€” read more about how to get started in the <a href=\"https://docs.smith.langchain.com\">docs</a>. \n",
    "     </p>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model and tools\n",
    "\n",
    "Let's first define the tools and model we will use for our example. As in the [ReAct agent guide](react-agent-from-scratch-functional.ipynb), we will use a single place-holder tool that gets a description of the weather for a location.\n",
    "\n",
    "We will use an [OpenAI](https://python.langchain.com/docs/integrations/providers/openai/) chat model for this example, but any model [supporting tool-calling](https://python.langchain.com/docs/integrations/chat/) will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str):\n",
    "    \"\"\"Call to get the weather from a specific location.\"\"\"\n",
    "    # This is a placeholder for the actual implementation\n",
    "    if any([city in location.lower() for city in [\"sf\", \"san francisco\"]]):\n",
    "        return \"It's sunny!\"\n",
    "    elif \"boston\" in location.lower():\n",
    "        return \"It's rainy!\"\n",
    "    else:\n",
    "        return f\"I am not sure what the weather is in {location}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @task\n",
    "# def call_tool(proposed_tool_call):\n",
    "#     review = review_tool_call(proposed_tool_call)\n",
    "#     if isinstance(review, ToolMessage):\n",
    "#         return review\n",
    "#     else:\n",
    "#         tool_call = review\n",
    "#         tool = tools_by_name[tool_call[\"name\"]]\n",
    "#         observation = tool.invoke(tool_call[\"args\"])\n",
    "#         return ToolMessage(content=observation, tool_call_id=tool_call[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reach out to a human for assistance\n",
    "\n",
    "To reach out to a human for assistance, we can simply add a tool that calls [interrupt](../../concepts/human_in_the_loop/#interrupt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "@tool\n",
    "def human_assistance(query: str) -> str:\n",
    "    \"\"\"Request assistance from a human.\"\"\"\n",
    "    human_response = interrupt({\"query\": query})\n",
    "    return human_response[\"data\"]\n",
    "\n",
    "\n",
    "tools = [get_weather, human_assistance]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tasks\n",
    "\n",
    "Our tasks are otherwise unchanged from the [ReAct agent guide](react-agent-from-scratch-functional.ipynb):\n",
    "\n",
    "1. **Call model**: We want to query our chat model with a list of messages.\n",
    "2. **Call tool**: If our model generates tool calls, we want to execute them.\n",
    "\n",
    "We just have one more tool accessible to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "from langgraph.func import entrypoint, task\n",
    "\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "\n",
    "@task\n",
    "def call_model(messages):\n",
    "    \"\"\"Call model with a sequence of messages.\"\"\"\n",
    "    response = model.bind_tools(tools).invoke(messages)\n",
    "    return response\n",
    "\n",
    "\n",
    "@task\n",
    "def call_tool(tool_call):\n",
    "    tool = tools_by_name[tool_call[\"name\"]]\n",
    "    observation = tool.invoke(tool_call[\"args\"])\n",
    "    return ToolMessage(content=observation, tool_call_id=tool_call[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define entrypoint\n",
    "\n",
    "Our [entrypoint](../../concepts/functional_api/#entrypoint) is also unchanged from the [ReAct agent guide](react-agent-from-scratch-functional.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "@entrypoint(checkpointer=checkpointer)\n",
    "def agent(messages, previous):\n",
    "    if previous is not None:\n",
    "        messages = add_messages(previous, messages)\n",
    "\n",
    "    llm_response = call_model(messages).result()\n",
    "    while True:\n",
    "        if not llm_response.tool_calls:\n",
    "            break\n",
    "\n",
    "        # Execute tools\n",
    "        tool_result_futures = [\n",
    "            call_tool(tool_call) for tool_call in llm_response.tool_calls\n",
    "        ]\n",
    "        tool_results = [fut.result() for fut in tool_result_futures]\n",
    "\n",
    "        # Append to message list\n",
    "        messages = add_messages(messages, [llm_response, *tool_results])\n",
    "\n",
    "        # Call model again\n",
    "        llm_response = call_model(messages).result()\n",
    "\n",
    "    # Generate final response\n",
    "    messages = add_messages(messages, llm_response)\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "Let's invoke our model with a question that requires human assistance. Our question will also require an invocation of the `get_weather` tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print_step(step: dict) -> None:\n",
    "    for task_name, result in step.items():\n",
    "        if task_name in (\"agent\", \"__interrupt__\"):\n",
    "            continue  # Just print task updates\n",
    "        else:\n",
    "            print(f\"\\n{task_name}:\")\n",
    "            result.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': 'Can you reach out for human assistance: what should I feed my cat? Separately, can you check the weather in San Francisco?'}\n",
      "\n",
      "call_model:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  human_assistance (call_Er3DsMcpdoO9yRy2NZHXV6mH)\n",
      " Call ID: call_Er3DsMcpdoO9yRy2NZHXV6mH\n",
      "  Args:\n",
      "    query: What should I feed my cat?\n",
      "  get_weather (call_vkBLyCTkmGR2vMndJyxnfzn7)\n",
      " Call ID: call_vkBLyCTkmGR2vMndJyxnfzn7\n",
      "  Args:\n",
      "    location: San Francisco\n",
      "\n",
      "call_tool:\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "It's sunny!\n"
     ]
    }
   ],
   "source": [
    "user_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": (\n",
    "        \"Can you reach out for human assistance: what should I feed my cat? \"\n",
    "        \"Separately, can you check the weather in San Francisco?\"\n",
    "    ),\n",
    "}\n",
    "print(user_message)\n",
    "\n",
    "for step in agent.stream([user_message], config):\n",
    "    _print_step(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we generate two tool calls, and although our run is interrupted, we did not block the execution of the `get_weather` tool.\n",
    "\n",
    "Let's inspect where we're interrupted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Interrupt(value={'query': 'What should I feed my cat?'}, resumable=True, ns=['agent:5e01f3a3-a097-3cb2-7557-b36fa4c5e427', 'call_tool:154fa07a-05dc-b025-2a4d-e7faa29c643a'], when='during'),)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_task = agent.get_state(config).tasks[0]\n",
    "next_task.interrupts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can resume execution by issuing a [Command](../../concepts/human_in_the_loop/#the-command-primitive). Note that the data we supply in the `Command` can be customized to your needs based on the implementation of `human_assistance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "call_tool:\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "You should feed your cat a fish.\n",
      "\n",
      "call_model:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I've reached out for human assistance regarding what to feed your cat, and the suggestion is to feed your cat fish. \n",
      "\n",
      "Additionally, the weather in San Francisco is sunny!\n"
     ]
    }
   ],
   "source": [
    "human_response = \"You should feed your cat a fish.\"\n",
    "human_command = Command(resume={\"data\": human_response})\n",
    "\n",
    "for step in agent.stream(human_command, config):\n",
    "    _print_step(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, when we resume we provide the final tool message, allowing the model to generate its response. Check out the LangSmith traces to see a full breakdown of the runs:\n",
    "\n",
    "1. [Trace from initial query](https://smith.langchain.com/public/c3d8879d-4d01-41be-807e-6d9eed15df99/r)\n",
    "2. [Trace after resuming](https://smith.langchain.com/public/97c05ef9-8b4c-428e-8826-3fd417c8c75f/r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review tool calls before execution\n",
    "\n",
    "To review tool calls before execution, we add a call to [interrupt](../../concepts/human_in_the_loop/#interrupt) to our [entrypoint](../../concepts/functional_api/#entrypoint) implementation. When this function is called, execution will be paused until we issue a command to resume it.\n",
    "\n",
    "The results of prior tasks-- in this case the initial model call-- are persisted, so that they are not run again following the `interrupt`.\n",
    "\n",
    "Let's first add a new `review_tool_call` task. Given a tool call, this will `interrupt` for human review. At that point we can either:\n",
    "\n",
    "- Accept the tool call;\n",
    "- Revise the tool call and continue;\n",
    "- Generate a custom tool message (e.g., instructing the model to re-format its tool call).\n",
    "\n",
    "We will demonstrate these three cases below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "from langchain_core.messages import ToolCall, ToolMessage\n",
    "from langgraph.func import entrypoint, task\n",
    "\n",
    "\n",
    "@task\n",
    "def review_tool_call(tool_call: ToolCall) -> Union[ToolCall, ToolMessage]:\n",
    "    \"\"\"Review a tool call, returning a validated version.\"\"\"\n",
    "    human_review = interrupt(\n",
    "        {\n",
    "            \"question\": \"Is this correct?\",\n",
    "            \"tool_call\": tool_call,\n",
    "        }\n",
    "    )\n",
    "    review_action = human_review[\"action\"]\n",
    "    review_data = human_review.get(\"data\")\n",
    "    if review_action == \"continue\":\n",
    "        return tool_call\n",
    "    elif review_action == \"update\":\n",
    "        updated_tool_call = {**tool_call, **{\"args\": review_data}}\n",
    "        return updated_tool_call\n",
    "    elif review_action == \"feedback\":\n",
    "        return ToolMessage(\n",
    "            content=review_data, name=tool_call[\"name\"], tool_call_id=tool_call[\"id\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, our tasks are the same. Let's subset to just the `get_weather` tool for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_weather]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "\n",
    "@task\n",
    "def call_model(messages):\n",
    "    \"\"\"Call model with a sequence of messages.\"\"\"\n",
    "    response = model.bind_tools(tools).invoke(messages)\n",
    "    return response\n",
    "\n",
    "\n",
    "@task\n",
    "def call_tool(tool_call):\n",
    "    tool = tools_by_name[tool_call[\"name\"]]\n",
    "    observation = tool.invoke(tool_call[\"args\"])\n",
    "    return ToolMessage(content=observation, tool_call_id=tool_call[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our tasks, we can update our entrypoint to review the generated tool calls. If a tool call is accepted or revised, we execute in the same way as before. Otherwise, we just append the `ToolMessage` supplied by the human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "@entrypoint(checkpointer=checkpointer)\n",
    "def agent(messages, previous):\n",
    "\n",
    "    if previous is not None:\n",
    "        messages = add_messages(previous, messages)\n",
    "\n",
    "    llm_response = call_model(messages).result()\n",
    "    while True:\n",
    "        if not llm_response.tool_calls:\n",
    "            break\n",
    "\n",
    "        # Review tool calls\n",
    "        tool_results = []\n",
    "        tool_calls = []\n",
    "        for tool_call in llm_response.tool_calls:\n",
    "            review = review_tool_call(tool_call).result()\n",
    "            if isinstance(review, ToolMessage):\n",
    "                tool_results.append(review)\n",
    "            else:\n",
    "                tool_calls.append(review)\n",
    "\n",
    "        # Execute remaining tool calls\n",
    "        tool_result_futures = [call_tool(tool_call) for tool_call in tool_calls]\n",
    "        remaining_tool_results = [fut.result() for fut in tool_result_futures]\n",
    "\n",
    "        # Append to message list\n",
    "        messages = add_messages(\n",
    "            messages,\n",
    "            [llm_response, *tool_results, *remaining_tool_results],\n",
    "        )\n",
    "\n",
    "        # Call model again\n",
    "        llm_response = call_model(messages).result()\n",
    "\n",
    "    # Generate final response\n",
    "    messages = add_messages(messages, llm_response)\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "Let's demonstrate some scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print_step(step: dict) -> None:\n",
    "    for task_name, result in step.items():\n",
    "        if task_name in (\"agent\", \"__interrupt__\"):\n",
    "            continue  # Just print task updates\n",
    "        else:\n",
    "            print(f\"\\n{task_name}:\")\n",
    "            if task_name == \"review_tool_call\":\n",
    "                print(result)\n",
    "            else:\n",
    "                result.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accept a tool call\n",
    "\n",
    "To accept a tool call, we just indicate in the data we provide in the `Command` that the tool call should pass through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': \"What's the weather in san francisco?\"}\n",
      "\n",
      "call_model:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_weather (call_sNICg8E5yx8vu8kbJuYdm4Vi)\n",
      " Call ID: call_sNICg8E5yx8vu8kbJuYdm4Vi\n",
      "  Args:\n",
      "    location: San Francisco\n"
     ]
    }
   ],
   "source": [
    "user_message = {\"role\": \"user\", \"content\": \"What's the weather in san francisco?\"}\n",
    "print(user_message)\n",
    "\n",
    "for step in agent.stream([user_message], config):\n",
    "    _print_step(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "review_tool_call:\n",
      "{'name': 'get_weather', 'args': {'location': 'San Francisco'}, 'id': 'call_sNICg8E5yx8vu8kbJuYdm4Vi', 'type': 'tool_call'}\n",
      "\n",
      "call_tool:\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "It's sunny!\n",
      "\n",
      "call_model:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The weather in San Francisco is sunny!\n"
     ]
    }
   ],
   "source": [
    "# highlight-next-line\n",
    "human_input = Command(resume={\"action\": \"continue\"})\n",
    "\n",
    "for step in agent.stream(human_input, config):\n",
    "    _print_step(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revise a tool call\n",
    "\n",
    "To revise a tool call, we can supply updated arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': \"What's the weather in san francisco?\"}\n",
      "\n",
      "call_model:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_weather (call_Yij7KP05J42rulHKlxqTSB2n)\n",
      " Call ID: call_Yij7KP05J42rulHKlxqTSB2n\n",
      "  Args:\n",
      "    location: san francisco\n"
     ]
    }
   ],
   "source": [
    "user_message = {\"role\": \"user\", \"content\": \"What's the weather in san francisco?\"}\n",
    "print(user_message)\n",
    "\n",
    "for step in agent.stream([user_message], config):\n",
    "    _print_step(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "review_tool_call:\n",
      "{'name': 'get_weather', 'args': {'location': 'boston'}, 'id': 'call_Yij7KP05J42rulHKlxqTSB2n', 'type': 'tool_call'}\n",
      "\n",
      "call_tool:\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "It's rainy!\n",
      "\n",
      "call_model:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The weather in San Francisco is rainy.\n"
     ]
    }
   ],
   "source": [
    "# highlight-next-line\n",
    "human_input = Command(resume={\"action\": \"update\", \"data\": {\"location\": \"boston\"}})\n",
    "\n",
    "for step in agent.stream(human_input, config):\n",
    "    _print_step(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a custom ToolMessage\n",
    "\n",
    "To Generate a custom `ToolMessage`, we supply the content of the message. In this case we will ask the model to reformat its tool call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': \"What's the weather in san francisco?\"}\n",
      "\n",
      "call_model:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_weather (call_XQtgUchIAkMeHUYAwtjvN7pg)\n",
      " Call ID: call_XQtgUchIAkMeHUYAwtjvN7pg\n",
      "  Args:\n",
      "    location: san francisco\n"
     ]
    }
   ],
   "source": [
    "user_message = {\"role\": \"user\", \"content\": \"What's the weather in san francisco?\"}\n",
    "print(user_message)\n",
    "\n",
    "for step in agent.stream([user_message], config):\n",
    "    _print_step(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "review_tool_call:\n",
      "content='Please format as <City>, <State>.' name='get_weather' id='76e420fd-301a-400d-a31c-41f43ee865a9' tool_call_id='call_rDf4c8Zsw1JzyLTrFcdl61bC'\n",
      "\n",
      "call_model:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_weather (call_vVsfdFZXisifRA9JliZKqHau)\n",
      " Call ID: call_vVsfdFZXisifRA9JliZKqHau\n",
      "  Args:\n",
      "    location: San Francisco, CA\n",
      "\n",
      "review_tool_call:\n",
      "content='Please format as <City>, <State>.' name='get_weather' tool_call_id='call_vVsfdFZXisifRA9JliZKqHau'\n",
      "\n",
      "call_model:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_weather (call_iarrPN17agVG6NByasbEC2kO)\n",
      " Call ID: call_iarrPN17agVG6NByasbEC2kO\n",
      "  Args:\n",
      "    location: San Francisco, California\n",
      "\n",
      "review_tool_call:\n",
      "content='Please format as <City>, <State>.' name='get_weather' tool_call_id='call_iarrPN17agVG6NByasbEC2kO'\n",
      "\n",
      "call_model:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_weather (call_UEIpANQOccZSpjXu0iivEngc)\n",
      " Call ID: call_UEIpANQOccZSpjXu0iivEngc\n",
      "  Args:\n",
      "    location: San Francisco, CA\n",
      "\n",
      "review_tool_call:\n",
      "content='Please format as <City>, <State>.' name='get_weather' tool_call_id='call_UEIpANQOccZSpjXu0iivEngc'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# highlight-next-line\n",
    "human_input = Command(\n",
    "    # highlight-next-line\n",
    "    resume={\n",
    "        # highlight-next-line\n",
    "        \"action\": \"feedback\",\n",
    "        # highlight-next-line\n",
    "        \"data\": \"Please format as <City>, <State>.\",\n",
    "    # highlight-next-line\n",
    "    },\n",
    "# highlight-next-line\n",
    ")\n",
    "\n",
    "for step in agent.stream(human_input, config):\n",
    "    _print_step(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it is re-formatted, we can accept it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "call_tool:\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "It's sunny!\n",
      "\n",
      "call_model:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The weather in San Francisco, CA is sunny!\n"
     ]
    }
   ],
   "source": [
    "# highlight-next-line\n",
    "human_input = Command(resume={\"action\": \"continue\"})\n",
    "\n",
    "for step in agent.stream(human_input, config):\n",
    "    _print_step(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
