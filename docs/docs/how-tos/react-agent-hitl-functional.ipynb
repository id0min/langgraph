{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to add human-in-the-loop processes to a ReAct agent (Functional API)\n",
    "\n",
    "!!! info \"Prerequisites\"\n",
    "    This guide assumes familiarity with the following:\n",
    "\n",
    "    - Implementing [human-in-the-loop](../../concepts/human_in_the_loop) workflows with [interrupt](../../concepts/human_in_the_loop/#interrupt)\n",
    "    - [How to create a ReAct agent using the Functional API](react-agent-from-scratch-functional.ipynb)\n",
    "\n",
    "This guide demonstrates how to implement human-in-the-loop workflows in a ReAct agent using the LangGraph [Functional API](../../concepts/functional_api).\n",
    "\n",
    "We will build off of the agent created in the [How to create a ReAct agent using the Functional API](react-agent-from-scratch-functional.ipynb) guide.\n",
    "\n",
    "Specifically, we will demonstrate:\n",
    "\n",
    "1. [How to review tool calls](#review-tool-calls-before-execution);\n",
    "2. How to allow the model to reach out to a human for assistance.\n",
    "\n",
    "Both of these goals can be accomplished through use of the [interrupt](../../concepts/human_in_the_loop/#interrupt) function at key points in our application.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install the required packages and set our API keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "     <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for better debugging</p>\n",
    "     <p style=\"padding-top: 5px;\">\n",
    "         Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM aps built with LangGraph â€” read more about how to get started in the <a href=\"https://docs.smith.langchain.com\">docs</a>. \n",
    "     </p>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model and tools\n",
    "\n",
    "Let's first define the tools and model we will use for our example. As in the [ReAct agent guide](react-agent-from-scratch-functional.ipynb), we will use a single place-holder tool that gets a description of the weather for a location.\n",
    "\n",
    "We will use an [OpenAI](https://python.langchain.com/docs/integrations/providers/openai/) chat model for this example, but any model [supporting tool-calling](https://python.langchain.com/docs/integrations/chat/) will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str):\n",
    "    \"\"\"Call to get the weather from a specific location.\"\"\"\n",
    "    # This is a placeholder for the actual implementation\n",
    "    if any([city in location.lower() for city in [\"sf\", \"san francisco\"]]):\n",
    "        return \"It's sunny!\"\n",
    "    elif \"boston\" in location.lower():\n",
    "        return \"It's rainy!\"\n",
    "    else:\n",
    "        return f\"I am not sure what the weather is in {location}\"\n",
    "\n",
    "\n",
    "tools = [get_weather]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tasks\n",
    "\n",
    "We start with the same tasks from the [ReAct agent guide](react-agent-from-scratch-functional.ipynb):\n",
    "\n",
    "1. **Call model**: We want to query our chat model with a list of messages.\n",
    "2. **Call tool**: If our model generates tool calls, we want to execute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "from langgraph.func import entrypoint, task\n",
    "\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "\n",
    "@task\n",
    "def call_model(messages):\n",
    "    \"\"\"Call model with a sequence of messages.\"\"\"\n",
    "    response = model.bind_tools(tools).invoke(messages)\n",
    "    return response\n",
    "\n",
    "\n",
    "@task\n",
    "def call_tool(tool_call):\n",
    "    tool = tools_by_name[tool_call[\"name\"]]\n",
    "    observation = tool.invoke(tool_call[\"args\"])\n",
    "    return ToolMessage(content=observation, tool_call_id=tool_call[\"id\"])\n",
    "\n",
    "\n",
    "# @task\n",
    "# def call_tool(proposed_tool_call):\n",
    "#     review = review_tool_call(proposed_tool_call)\n",
    "#     if isinstance(review, ToolMessage):\n",
    "#         return review\n",
    "#     else:\n",
    "#         tool_call = review\n",
    "#         tool = tools_by_name[tool_call[\"name\"]]\n",
    "#         observation = tool.invoke(tool_call[\"args\"])\n",
    "#         return ToolMessage(content=observation, tool_call_id=tool_call[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review tool calls before execution\n",
    "\n",
    "To review tool calls before execution, we add a call to [interrupt](../../concepts/human_in_the_loop/#interrupt) to our [entrypoint](../../concepts/functional_api/#entrypoint) implementation. When this function is called, execution will be paused until we issue a command to resume it.\n",
    "\n",
    "The results of prior tasks-- in this case the initial model call-- are persisted, so that they are not run again following the `interrupt`.\n",
    "\n",
    "Aside from this addition, our entrypoint implementation is the same as in the [ReAct agent guide](react-agent-from-scratch-functional.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from langchain_core.messages import ToolCall, ToolMessage\n",
    "\n",
    "\n",
    "@task\n",
    "def review_tool_call(tool_call: ToolCall) -> Union[ToolCall, ToolMessage]:\n",
    "    \"\"\"Review a tool call, returning a validated version.\"\"\"\n",
    "    human_review = interrupt(\n",
    "        {\n",
    "            \"question\": \"Is this correct?\",\n",
    "            \"tool_call\": tool_call,\n",
    "        }\n",
    "    )\n",
    "    review_action = human_review[\"action\"]\n",
    "    review_data = human_review.get(\"data\")\n",
    "    if review_action == \"continue\":\n",
    "        return tool_call\n",
    "    elif review_action == \"update\":\n",
    "        updated_tool_call = {**tool_call, **{\"args\": review_data}}\n",
    "        return updated_tool_call\n",
    "    elif review_action == \"feedback\":\n",
    "        return ToolMessage(\n",
    "            content=review_data, name=tool_call[\"name\"], tool_call_id=tool_call[\"id\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "@entrypoint(checkpointer=checkpointer)\n",
    "def agent(messages, previous):\n",
    "\n",
    "    if previous is not None:\n",
    "        messages = add_messages(previous, messages)\n",
    "\n",
    "    llm_response = call_model(messages).result()\n",
    "    while True:\n",
    "        if not llm_response.tool_calls:\n",
    "            break\n",
    "\n",
    "        # Review tool calls\n",
    "        tool_results = []\n",
    "        tool_calls = []\n",
    "        for tool_call in llm_response.tool_calls:\n",
    "            review = review_tool_call(tool_call).result()\n",
    "            if isinstance(review, ToolMessage):\n",
    "                tool_results.append(review)\n",
    "            else:\n",
    "                tool_calls.append(review)\n",
    "\n",
    "        # Execute remaining tool calls\n",
    "        tool_result_futures = [call_tool(tool_call) for tool_call in tool_calls]\n",
    "        remaining_tool_results = [fut.result() for fut in tool_result_futures]\n",
    "\n",
    "        # Append to message list\n",
    "        messages = add_messages(\n",
    "            messages,\n",
    "            [llm_response, *tool_results, *remaining_tool_results],\n",
    "        )\n",
    "\n",
    "        # Call model again\n",
    "        llm_response = call_model(messages).result()\n",
    "\n",
    "    # Generate final response\n",
    "    messages = add_messages(messages, llm_response)\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "To use our agent, we invoke it with a messages list. Based on our implementation, these can be LangChain [message](https://python.langchain.com/docs/concepts/messages/) objects or OpenAI-style dicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print_step(step: dict) -> None:\n",
    "    for task_name, result in step.items():\n",
    "        if task_name in (\"agent\", \"__interrupt__\"):\n",
    "            continue  # Just print task updates\n",
    "        else:\n",
    "            print(f\"\\n{task_name}:\")\n",
    "            if task_name == \"review_tool_call\":\n",
    "                print(result)\n",
    "            else:\n",
    "                result.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accept a tool call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': \"What's the weather in san francisco?\"}\n",
      "\n",
      "call_model:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_weather (call_P89CnI8LgJYwoEo5KAjoxDef)\n",
      " Call ID: call_P89CnI8LgJYwoEo5KAjoxDef\n",
      "  Args:\n",
      "    location: San Francisco\n"
     ]
    }
   ],
   "source": [
    "user_message = {\"role\": \"user\", \"content\": \"What's the weather in san francisco?\"}\n",
    "print(user_message)\n",
    "\n",
    "for step in agent.stream([user_message], config):\n",
    "    _print_step(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "review_tool_call:\n",
      "{'name': 'get_weather', 'args': {'location': 'San Francisco'}, 'id': 'call_P89CnI8LgJYwoEo5KAjoxDef', 'type': 'tool_call'}\n",
      "\n",
      "call_tool:\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "It's sunny!\n",
      "\n",
      "call_model:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The weather in San Francisco is sunny!\n"
     ]
    }
   ],
   "source": [
    "# highlight-next-line\n",
    "human_input = Command(resume={\"action\": \"continue\"})\n",
    "\n",
    "for step in agent.stream(human_input, config):\n",
    "    _print_step(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revise a tool call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': \"What's the weather in san francisco?\"}\n",
      "\n",
      "call_model:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_weather (call_k4LihRKjdhBneMQPzfyOfK6G)\n",
      " Call ID: call_k4LihRKjdhBneMQPzfyOfK6G\n",
      "  Args:\n",
      "    location: San Francisco\n"
     ]
    }
   ],
   "source": [
    "user_message = {\"role\": \"user\", \"content\": \"What's the weather in san francisco?\"}\n",
    "print(user_message)\n",
    "\n",
    "for step in agent.stream([user_message], config):\n",
    "    _print_step(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "review_tool_call:\n",
      "{'name': 'get_weather', 'args': {'location': 'boston'}, 'id': 'call_k4LihRKjdhBneMQPzfyOfK6G', 'type': 'tool_call'}\n",
      "\n",
      "call_tool:\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "It's rainy!\n",
      "\n",
      "call_model:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The weather in San Francisco is rainy!\n"
     ]
    }
   ],
   "source": [
    "# highlight-next-line\n",
    "human_input = Command(resume={\"action\": \"update\", \"data\": {\"location\": \"boston\"}})\n",
    "\n",
    "for step in agent.stream(human_input, config):\n",
    "    _print_step(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a custom ToolMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': \"What's the weather in san francisco?\"}\n",
      "\n",
      "call_model:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_weather (call_Qimc3WebYD8MIh8NWf47I4EM)\n",
      " Call ID: call_Qimc3WebYD8MIh8NWf47I4EM\n",
      "  Args:\n",
      "    location: San Francisco\n"
     ]
    }
   ],
   "source": [
    "user_message = {\"role\": \"user\", \"content\": \"What's the weather in san francisco?\"}\n",
    "print(user_message)\n",
    "\n",
    "for step in agent.stream([user_message], config):\n",
    "    _print_step(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "review_tool_call:\n",
      "content='Please format as <City>, <State>.' name='get_weather' id='49403506-0862-45ff-a8a8-9d94a8564155' tool_call_id='call_Qimc3WebYD8MIh8NWf47I4EM'\n",
      "\n",
      "call_model:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_weather (call_X03tBLOYU4P7sQ0xKci4dAv7)\n",
      " Call ID: call_X03tBLOYU4P7sQ0xKci4dAv7\n",
      "  Args:\n",
      "    location: San Francisco, CA\n",
      "\n",
      "review_tool_call:\n",
      "content='Please format as <City>, <State>.' name='get_weather' tool_call_id='call_X03tBLOYU4P7sQ0xKci4dAv7'\n",
      "\n",
      "call_model:\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_weather (call_Pt4j9FfxqwXf6S9NNdJ88oxN)\n",
      " Call ID: call_Pt4j9FfxqwXf6S9NNdJ88oxN\n",
      "  Args:\n",
      "    location: San Francisco, California\n",
      "\n",
      "review_tool_call:\n",
      "content='Please format as <City>, <State>.' name='get_weather' tool_call_id='call_Pt4j9FfxqwXf6S9NNdJ88oxN'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WeakKeyDictionary.__init__.<locals>.remove at 0x1054ae200>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/chestercurme/.pyenv/versions/3.10.4/lib/python3.10/weakref.py\", line 370, in remove\n",
      "    def remove(k, selfref=ref(self)):\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/langgraph/libs/langgraph/langgraph/pregel/manager.py:37\u001b[0m, in \u001b[0;36mChannelsManager\u001b[0;34m(specs, checkpoint, loop, skip_context)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ExitStack() \u001b[38;5;28;01mas\u001b[39;00m stack:\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (\n\u001b[1;32m     38\u001b[0m         {\n\u001b[1;32m     39\u001b[0m             k: v\u001b[38;5;241m.\u001b[39mfrom_checkpoint(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(k))\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m channel_specs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     41\u001b[0m         },\n\u001b[1;32m     42\u001b[0m         ManagedValueMapping(\n\u001b[1;32m     43\u001b[0m             {\n\u001b[1;32m     44\u001b[0m                 key: stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[1;32m     45\u001b[0m                     value\u001b[38;5;241m.\u001b[39mcls\u001b[38;5;241m.\u001b[39menter(loop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvalue\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m                     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ConfiguredManagedValue)\n\u001b[1;32m     47\u001b[0m                     \u001b[38;5;28;01melse\u001b[39;00m value\u001b[38;5;241m.\u001b[39menter(loop)\n\u001b[1;32m     48\u001b[0m                 )\n\u001b[1;32m     49\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m managed_specs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     50\u001b[0m             }\n\u001b[1;32m     51\u001b[0m         ),\n\u001b[1;32m     52\u001b[0m     )\n",
      "File \u001b[0;32m~/repos/langgraph/libs/langgraph/langgraph/pregel/__init__.py:1670\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1670\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1671\u001b[0m         loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1672\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1673\u001b[0m         retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1674\u001b[0m         get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1675\u001b[0m     ):\n\u001b[1;32m   1676\u001b[0m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1677\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m output()\n",
      "File \u001b[0;32m~/repos/langgraph/libs/langgraph/langgraph/pregel/runner.py:275\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(futures) \u001b[38;5;241m>\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_waiter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 275\u001b[0m     done, inflight \u001b[38;5;241m=\u001b[39m \u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_when\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIRST_COMPLETED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonotonic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/concurrent/futures/_base.py:307\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(fs, timeout, return_when)\u001b[0m\n\u001b[1;32m    305\u001b[0m     waiter \u001b[38;5;241m=\u001b[39m _create_and_install_waiters(fs, return_when)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fs:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/threading.py:600\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 600\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 14\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# highlight-next-line\u001b[39;00m\n\u001b[1;32m      2\u001b[0m human_input \u001b[38;5;241m=\u001b[39m Command(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# highlight-next-line\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     resume\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# highlight-next-line\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mstream(human_input, config):\n\u001b[1;32m     15\u001b[0m     _print_step(step)\n",
      "File \u001b[0;32m~/repos/langgraph/libs/langgraph/langgraph/pregel/__init__.py:1614\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stream_modes:\n\u001b[1;32m   1611\u001b[0m     config[CONF][CONFIG_KEY_STREAM_WRITER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m c: stream\u001b[38;5;241m.\u001b[39mput(\n\u001b[1;32m   1612\u001b[0m         ((), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m, c)\n\u001b[1;32m   1613\u001b[0m     )\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SyncPregelLoop(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1616\u001b[0m     stream\u001b[38;5;241m=\u001b[39mStreamProtocol(stream\u001b[38;5;241m.\u001b[39mput, stream_modes),\n\u001b[1;32m   1617\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m   1618\u001b[0m     store\u001b[38;5;241m=\u001b[39mstore,\n\u001b[1;32m   1619\u001b[0m     checkpointer\u001b[38;5;241m=\u001b[39mcheckpointer,\n\u001b[1;32m   1620\u001b[0m     nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes,\n\u001b[1;32m   1621\u001b[0m     specs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels,\n\u001b[1;32m   1622\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1623\u001b[0m     stream_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_channels_asis,\n\u001b[1;32m   1624\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   1625\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   1626\u001b[0m     manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1627\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1628\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m loop:\n\u001b[1;32m   1629\u001b[0m     \u001b[38;5;66;03m# create runner\u001b[39;00m\n\u001b[1;32m   1630\u001b[0m     runner \u001b[38;5;241m=\u001b[39m PregelRunner(\n\u001b[1;32m   1631\u001b[0m         submit\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39msubmit,\n\u001b[1;32m   1632\u001b[0m         put_writes\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39mput_writes,\n\u001b[1;32m   1633\u001b[0m         schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[1;32m   1634\u001b[0m         node_finished\u001b[38;5;241m=\u001b[39mconfig[CONF]\u001b[38;5;241m.\u001b[39mget(CONFIG_KEY_NODE_FINISHED),\n\u001b[1;32m   1635\u001b[0m     )\n\u001b[1;32m   1636\u001b[0m     \u001b[38;5;66;03m# enable subgraph streaming\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/langgraph/libs/langgraph/langgraph/pregel/loop.py:940\u001b[0m, in \u001b[0;36mSyncPregelLoop.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    935\u001b[0m     exc_type: Optional[Type[\u001b[38;5;167;01mBaseException\u001b[39;00m]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    938\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mbool\u001b[39m]:\n\u001b[1;32m    939\u001b[0m     \u001b[38;5;66;03m# unwind stack\u001b[39;00m\n\u001b[0;32m--> 940\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/contextlib.py:576\u001b[0m, in \u001b[0;36mExitStack.__exit__\u001b[0;34m(self, *exc_details)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# bare \"raise exc_details[1]\" replaces our carefully\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;66;03m# set-up context\u001b[39;00m\n\u001b[1;32m    575\u001b[0m     fixed_ctx \u001b[38;5;241m=\u001b[39m exc_details[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39m__context__\n\u001b[0;32m--> 576\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_details[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    578\u001b[0m     exc_details[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m fixed_ctx\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/contextlib.py:561\u001b[0m, in \u001b[0;36mExitStack.__exit__\u001b[0;34m(self, *exc_details)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m is_sync\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexc_details\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    562\u001b[0m         suppressed_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    563\u001b[0m         pending_raise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/langgraph/libs/langgraph/langgraph/pregel/executor.py:110\u001b[0m, in \u001b[0;36mBackgroundExecutor.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# wait for all tasks to finish\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pending \u001b[38;5;241m:=\u001b[39m {t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tasks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdone()}:\n\u001b[0;32m--> 110\u001b[0m     \u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpending\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# shutdown the executor\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(exc_type, exc_value, traceback)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/concurrent/futures/_base.py:307\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(fs, timeout, return_when)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m DoneAndNotDoneFutures(done, not_done)\n\u001b[1;32m    305\u001b[0m     waiter \u001b[38;5;241m=\u001b[39m _create_and_install_waiters(fs, return_when)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fs:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m f\u001b[38;5;241m.\u001b[39m_condition:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/threading.py:600\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    598\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 600\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# highlight-next-line\n",
    "human_input = Command(\n",
    "    # highlight-next-line\n",
    "    resume={\n",
    "        # highlight-next-line\n",
    "        \"action\": \"feedback\",\n",
    "        # highlight-next-line\n",
    "        \"data\": \"Please format as <City>, <State>.\",\n",
    "    # highlight-next-line\n",
    "    },\n",
    "# highlight-next-line\n",
    ")\n",
    "\n",
    "for step in agent.stream(human_input, config):\n",
    "    _print_step(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# highlight-next-line\n",
    "human_input = Command(resume={\"action\": \"continue\"})\n",
    "\n",
    "for step in agent.stream(human_input, config):\n",
    "    _print_step(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
