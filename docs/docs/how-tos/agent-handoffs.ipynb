{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34d3d54e-9a2b-481e-bccd-74aca7a53f9a",
   "metadata": {},
   "source": [
    "# How to implement handoffs between agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef16392a-56de-4cda-9ae8-dff078b2ed87",
   "metadata": {},
   "source": [
    "!!! info \"Prerequisites\"\n",
    "    This guide assumes familiarity with the following:\n",
    "\n",
    "    - [Multi-agent systems](../../concepts/multi_agent)\n",
    "    - [Command](../../concepts/low_level/#command)\n",
    "    - [LangGraph Glossary](../../concepts/low_level/)\n",
    "    \n",
    "\n",
    "In multi-agent architectures, agents can be represented as graph nodes. Each agent node executes its step(s) and decides whether to finish execution or route to another agent, including potentially routing to itself (e.g., running in a loop). A common pattern in multi-agent interactions is [handoffs](../../concepts/multi_agent#handoffs), where one agent hands off control to another. Handoffs allow you to specify:\n",
    "\n",
    "- **destination**: target agent to navigate to (e.g., name of the node to go to)\n",
    "- **payload**: information to pass to that agent (e.g., state update)\n",
    "\n",
    "To implement handoffs in LangGraph, agent nodes can return `Command` object that allows you to [combine both control flow and state updates](../command):\n",
    "\n",
    "```python\n",
    "def agent(state) -> Command[Literal[\"agent\", \"another_agent\"]]:\n",
    "    # the condition for routing/halting can be anything, e.g. LLM tool call / structured output, etc.\n",
    "    goto = get_next_agent(...)  # 'agent' / 'another_agent'\n",
    "    return Command(\n",
    "        # Specify which agent to call next\n",
    "        goto=goto,\n",
    "        # Update the graph state\n",
    "        update={\"my_state_key\": \"my_state_value\"}\n",
    "    )\n",
    "```\n",
    "\n",
    "One of the most common agent types is a ReAct-style tool-calling agents. For those types of agents, a common pattern is wrapping a handoff in a tool call, e.g.:\n",
    "\n",
    "```python\n",
    "def transfer_to_bob(state):\n",
    "    \"\"\"Transfer to bob.\"\"\"\n",
    "    return Command(\n",
    "        goto=\"bob\",\n",
    "        update={\"my_state_key\": \"my_state_value\"},\n",
    "        graph=Command.PARENT,\n",
    "    )\n",
    "```\n",
    "\n",
    "This guide shows how you can implement:\n",
    "\n",
    "- **custom handoffs**: agent node makes some decision (usually LLM-based), and explicitly returns a handoff via `Command`. These are useful if you need a fine-grained control over how an agent routes to a different agent. It could be well suited for implementing a supervisor agent in a supervisor architecture.\n",
    "- a special **handoff tool**: a tool-calling agent has access to tools that can return a handoff via `Command`. Tool-executing node in the agent recognizes the `Command` objects returned by the tools and routes control flow accordingly. It's a general-purpose primitive that is useful in any multi-agent systems that contain tool-calling agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4274c8-204f-41e4-b7ef-c8d1bb8de02e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e060e7a2-e339-49a6-bfd0-071dba8a3131",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4864843-00a1-4c88-9a7c-c34e6c31c548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230aec8a-ed82-4b97-a52e-2131f6c295ed",
   "metadata": {},
   "source": [
    "<div class=\"admonition tip\">\n",
    "    <p class=\"admonition-title\">Set up <a href=\"https://smith.langchain.com\">LangSmith</a> for LangGraph development</p>\n",
    "    <p style=\"padding-top: 5px;\">\n",
    "        Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started <a href=\"https://docs.smith.langchain.com\">here</a>. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4157f016-ccce-4f3a-877c-d3b3cfd77ffe",
   "metadata": {},
   "source": [
    "## Custom handoffs using `Command`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a75f43-cf79-4e5d-b2b2-fc8982bc84a8",
   "metadata": {},
   "source": [
    "Let's implement a system with two agents:\n",
    "\n",
    "- an addition expert (can only add numbers)\n",
    "- a multiplication expert (can only multiply numbers).\n",
    "\n",
    "In this example the agents will be relying on the LLM for doing math. In the following example, we will give the agents tools for doing math.\n",
    "\n",
    "When the addition expert needs help with multiplication, it hands off to the multiplication expert and vice-versa. This is an example of a simple multi-agent network.\n",
    "\n",
    "Each agent will have a corresponding node function that can conditionally return a `Command` object (e.g. our handoff). The node function will contain a single LLM call with a system prompt and a single tool. This tool will allow LLM to notify us that it needs to transfer to the other agent. If the LLM responds with the tool calls, we will return a `Command(goto=<other_agent>)`.\n",
    "\n",
    "**Note**: while we're using tools for the LLM to signal that it needs a handoff, the condition for the handoff can be anything: a specific response text from the LLM, structured output from the LLM, any other custom logic, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e184beb-b9b2-4bd0-ac35-0356a8da46bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import MessagesState, StateGraph, START\n",
    "from langgraph.types import Command\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def transfer_to_multiplication_expert():\n",
    "    \"\"\"Ask multiplication agent for help.\"\"\"\n",
    "    return\n",
    "\n",
    "\n",
    "@tool\n",
    "def transfer_to_addition_expert():\n",
    "    \"\"\"Ask addition agent for help.\"\"\"\n",
    "    return\n",
    "\n",
    "\n",
    "def addition_expert(state: MessagesState) -> Command[Literal[\"multiplication_expert\"]]:\n",
    "    system_prompt = (\n",
    "        \"You are an addition expert, you can ask the multiplication expert for help with multiplication. \"\n",
    "        \"Always do your portion of calculation before the handoff.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    ai_msg = model.bind_tools([transfer_to_multiplication_expert]).invoke(messages)\n",
    "    ai_msg.name = \"addition_expert\"\n",
    "    # If there are tool calls, the LLM needs to hand off to another agent\n",
    "    if len(ai_msg.tool_calls) > 0:\n",
    "        tool_call_id = ai_msg.tool_calls[-1][\"id\"]\n",
    "        # NOTE: it's important to insert a tool message here because LLM providers are expecting\n",
    "        # all AI messages to be followed by a corresponding tool result message\n",
    "        tool_msg = {\"role\": \"tool\", \"content\": \"Successfully transferred\", \"tool_call_id\": tool_call_id}\n",
    "        return Command(goto=\"multiplication_expert\", update={\"messages\": [ai_msg, tool_msg]})\n",
    "\n",
    "    # If the expert has an answer, return it directly to the user\n",
    "    return {\"messages\": [ai_msg]}\n",
    "\n",
    "\n",
    "def multiplication_expert(state: MessagesState) -> Command[Literal[\"addition_expert\"]]:\n",
    "    system_prompt = (\n",
    "        \"You are a multiplication expert, you can ask an addition expert for help with addition. \"\n",
    "        \"Always do your portion of calculation before the handoff.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    ai_msg = model.bind_tools([transfer_to_addition_expert]).invoke(messages)\n",
    "    ai_msg.name = \"multiplication_expert\"\n",
    "    if len(ai_msg.tool_calls) > 0:\n",
    "        tool_call_id = ai_msg.tool_calls[-1][\"id\"]\n",
    "        tool_msg = {\"role\": \"tool\", \"content\": \"Successfully transferred\", \"tool_call_id\": tool_call_id}\n",
    "        return Command(goto=\"addition_expert\", update={\"messages\": [ai_msg, tool_msg]})\n",
    "\n",
    "    return {\"messages\": [ai_msg]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921dc7bb-0c5b-410e-b143-601a549d529d",
   "metadata": {},
   "source": [
    "Let's now combine both of these nodes into a single graph. Note that there are no edges between the agents! If the expert has an answer, it will return it directly to the user, otherwise it will route to the other expert for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f56b6617-4226-4a7f-8234-59ebeaf53447",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"addition_expert\", addition_expert)\n",
    "builder.add_node(\"multiplication_expert\", multiplication_expert)\n",
    "# we'll always start with the addition expert\n",
    "builder.add_edge(START, \"addition_expert\")\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36574f3-7990-4ef2-b556-3bbd3625ec17",
   "metadata": {},
   "source": [
    "Let's run the graph with an expression that requires both addition and multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62ba5113-e972-41e6-8392-cc970d4eea72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'addition_expert': {'messages': [AIMessage(content=\"First, I'll calculate the addition part: \\\\(3 + 5 = 8\\\\). \\n\\nNow, I will ask the multiplication expert to help with \\\\(8 \\\\times 12\\\\).\", additional_kwargs={'tool_calls': [{'id': 'call_6uwHIwFxoWYwVkiXbnIsQlsU', 'function': {'arguments': '{}', 'name': 'transfer_to_multiplication_expert'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 81, 'total_tokens': 135, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a79d8dac1f', 'finish_reason': 'tool_calls', 'logprobs': None}, name='addition_expert', id='run-a79c7ba7-7850-4995-8cd0-12f8b7073217-0', tool_calls=[{'name': 'transfer_to_multiplication_expert', 'args': {}, 'id': 'call_6uwHIwFxoWYwVkiXbnIsQlsU', 'type': 'tool_call'}], usage_metadata={'input_tokens': 81, 'output_tokens': 54, 'total_tokens': 135, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), {'role': 'tool', 'content': 'Successfully transferred', 'tool_call_id': 'call_6uwHIwFxoWYwVkiXbnIsQlsU'}]}}\n",
      "\n",
      "\n",
      "{'multiplication_expert': {'messages': [AIMessage(content='The result of \\\\((3 + 5) \\\\times 12\\\\) is \\\\(96\\\\).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 160, 'total_tokens': 182, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_9faba9f038', 'finish_reason': 'stop', 'logprobs': None}, name='multiplication_expert', id='run-09b0eb24-86e5-47cf-a23f-019df8fd2d5a-0', usage_metadata={'input_tokens': 160, 'output_tokens': 22, 'total_tokens': 182, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"what's (3 + 5) * 12\"\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0088d791-1e03-49fb-b640-0ea01a7ef61d",
   "metadata": {},
   "source": [
    "You can see that the addition expert first handled the expression in the parentheses, and then handed off to the multiplication expert to finish the calculation.\n",
    "\n",
    "Now let's see how we can implement this same system using special handoff tools and give our agents actual math tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a5d161-9f74-47a2-83ce-0cbe6073edcc",
   "metadata": {},
   "source": [
    "## Handoff tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccfbd12-0a27-4e34-b6f7-701c2e7a6ffb",
   "metadata": {},
   "source": [
    "### Implementing a handoff tool"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acaed37a-ddd3-4bf9-ac30-a9c5cc1ea3fe",
   "metadata": {},
   "source": [
    "In the previous example we explicitly defined custom handoffs in each of the agent nodes. Another pattern is to create special **handoff tools** that directly return `Command` objects. When an agent calls a tool like this, it hands the control off to a different agent. Specifically, the tool-executing node in the agent recognizes the `Command` objects returned by the tools and routes control flow accordingly. **Note**: unlike the previous example, a tool-calling agent is not a single node but another graph that can be added to the multi-agent graph as a subgraph node.\n",
    "\n",
    "There are a few important considerations when implementing handoff tools:\n",
    "\n",
    "- since each agent is a __subgraph__ node in another graph, and the tools will be called in one of the agent subgraph nodes (e.g. tool executor), we need to specify `graph=Command.PARENT` in the `Command`, so that LangGraph knows to navigate outside of the agent subgraph\n",
    "- we can optionally specify a state update that will be applied to the parent graph state before the next agent is called\n",
    "- we can optionally provide the following to the tool (in the tool function signature):\n",
    "  \n",
    "    - graph state (using `InjectedState`)\n",
    "    - graph long-term memory (using `InjectedStore`)\n",
    "    - the current tool call ID (using `InjectedToolCallId`)\n",
    "      \n",
    "    These are not necessary but are useful for creating the state update passed to the next agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d022072b-39bf-4133-aa62-e20f22bb4b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.tools.base import InjectedToolCallId\n",
    "from langgraph.prebuilt import InjectedState\n",
    "\n",
    "\n",
    "def make_handoff_tool(*, agent_name: str):\n",
    "    \"\"\"Create a tool that can return handoff via a Command\"\"\"\n",
    "    tool_name = f\"transfer_to_{agent_name}\"\n",
    "\n",
    "    @tool(tool_name)\n",
    "    def handoff_to_agent(\n",
    "        # # optionally pass current graph state to the tool (will be ignored by the LLM)\n",
    "        state: Annotated[dict, InjectedState],\n",
    "        # optionally pass the current tool call ID (will be ignored by the LLM)\n",
    "        tool_call_id: Annotated[str, InjectedToolCallId],\n",
    "    ):\n",
    "        \"\"\"Ask another agent for help.\"\"\"\n",
    "        tool_message = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": f\"Successfully transferred to {agent_name}\",\n",
    "            \"name\": tool_name,\n",
    "            \"tool_call_id\": tool_call_id,\n",
    "        }\n",
    "        return Command(\n",
    "            # navigate to another agent node in the PARENT graph\n",
    "            goto=agent_name,\n",
    "            graph=Command.PARENT,\n",
    "            # This is the state update that the agent `agent_name` will see when it is invoked.\n",
    "            # we're passing the full list of messages (agent's internal scratchpad) AND adding a tool message to make sure\n",
    "            # the resulting chat history is valid (AI messages w/ tool calls are followed by the tool result messages).\n",
    "            # NOTE: this is not necessary for implementing multi-agent architectures. You can control how much information to share\n",
    "            # between the agents (full state / last message, etc). Just remember that the resulting message history needs to be valid.\n",
    "            update={\"messages\": state[\"messages\"] + [tool_message]},\n",
    "        )\n",
    "\n",
    "    return handoff_to_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba83e85a-576a-4e3f-8086-ce5e2c149c60",
   "metadata": {},
   "source": [
    "### Using in a custom agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d263776-3b2d-4ab0-9cc8-c7cbef22e2d5",
   "metadata": {},
   "source": [
    "To demonstrate how to use handoff tools, let's first implement a simple version of the prebuilt [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent]. This is useful in case you want to have a custom tool-calling agent implementation and want to leverage handoff tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5cd13c5-7dac-4dd7-9ffc-9f3467e28a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import MessagesState, StateGraph, START\n",
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "def make_agent(model, tools, system_prompt=None):\n",
    "    model_with_tools = model.bind_tools(tools, parallel_tool_calls=False)\n",
    "    tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def call_model(state: MessagesState) -> Command[Literal[\"call_tools\", \"__end__\"]]:\n",
    "        messages = state[\"messages\"]\n",
    "        if system_prompt:\n",
    "            messages = [{\"role\": \"system\", \"content\": system_prompt}] + messages\n",
    "\n",
    "        response = model_with_tools.invoke(messages)\n",
    "        if len(response.tool_calls) > 0:\n",
    "            return Command(goto=\"call_tools\", update={\"messages\": response})\n",
    "\n",
    "        return {\"messages\": response}\n",
    "\n",
    "    # NOTE: this is a simplified version of the prebuilt ToolNode\n",
    "    # If you want to have a tool node that has full feature parity, please refer to the source code\n",
    "    def call_tools(state: MessagesState) -> Command[Literal[\"call_model\"]]:\n",
    "        tool_calls = state[\"messages\"][-1].tool_calls\n",
    "        results = []\n",
    "        for tool_call in tool_calls:\n",
    "            tool_ = tools_by_name[tool_call[\"name\"]]\n",
    "            tool_input_fields = tool_.get_input_schema().model_json_schema()[\"properties\"]\n",
    "\n",
    "            # this is simplified for demonstration purposes and\n",
    "            # is different from the ToolNode implementation\n",
    "            if \"state\" in tool_input_fields:\n",
    "                # inject state\n",
    "                tool_call = {**tool_call, \"args\": {**tool_call[\"args\"], \"state\": state}}\n",
    "\n",
    "            tool_response = tool_.invoke(tool_call)\n",
    "            if isinstance(tool_response, ToolMessage):\n",
    "                results.append(Command(goto=\"call_model\", update={\"messages\": tool_response}))\n",
    "\n",
    "            # handle tools that return Command directly\n",
    "            elif isinstance(tool_response, Command):\n",
    "                results.append(tool_response)\n",
    "\n",
    "        # NOTE: nodes in LangGraph allow you to return list of updates, including Command objects\n",
    "        return results\n",
    "\n",
    "    graph = StateGraph(MessagesState)\n",
    "    graph.add_node(call_model)\n",
    "    graph.add_node(call_tools)\n",
    "    graph.add_edge(START, \"call_model\")\n",
    "\n",
    "    return graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7231d5-1e01-41a7-b260-d0495323d552",
   "metadata": {},
   "source": [
    "Let's also define math tools that we'll give our agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9f8e553-8894-4d59-a069-1baa05d23289",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies two numbers.\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f76999-2364-4e08-90be-62d4d138a8f4",
   "metadata": {},
   "source": [
    "Let's test the agent implementation out to make sure it's working as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f39f99b0-95c7-422f-96a6-e612fde186df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'call_model': {'messages': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_dinLzcw8RWZcrmCOEO6rd6PV', 'function': {'arguments': '{\"a\":3,\"b\":5}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 78, 'total_tokens': 95, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a79d8dac1f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f0d0fe77-9c49-453c-8fc1-515bc4355cc1-0', tool_calls=[{'name': 'add', 'args': {'a': 3, 'b': 5}, 'id': 'call_dinLzcw8RWZcrmCOEO6rd6PV', 'type': 'tool_call'}], usage_metadata={'input_tokens': 78, 'output_tokens': 17, 'total_tokens': 95, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}}\n",
      "\n",
      "\n",
      "{'call_tools': {'messages': ToolMessage(content='8', name='add', id='a4a7482e-2ba6-4997-82fa-f799ea53d7f6', tool_call_id='call_dinLzcw8RWZcrmCOEO6rd6PV')}}\n",
      "\n",
      "\n",
      "{'call_model': {'messages': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_FCKDyB0xt8DKknIXHp4XHKfx', 'function': {'arguments': '{\"a\":8,\"b\":12}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 103, 'total_tokens': 120, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a79d8dac1f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-894d6114-be2d-4e71-8bc3-2e7d07c0a862-0', tool_calls=[{'name': 'multiply', 'args': {'a': 8, 'b': 12}, 'id': 'call_FCKDyB0xt8DKknIXHp4XHKfx', 'type': 'tool_call'}], usage_metadata={'input_tokens': 103, 'output_tokens': 17, 'total_tokens': 120, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}}\n",
      "\n",
      "\n",
      "{'call_tools': {'messages': ToolMessage(content='96', name='multiply', id='8beae2e7-abee-4f94-b65f-dabb6d463795', tool_call_id='call_FCKDyB0xt8DKknIXHp4XHKfx')}}\n",
      "\n",
      "\n",
      "{'call_model': {'messages': AIMessage(content='The result of \\\\((3 + 5) \\\\times 12\\\\) is 96.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 128, 'total_tokens': 149, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_9faba9f038', 'finish_reason': 'stop', 'logprobs': None}, id='run-eee0c6f3-3fb9-4225-b11a-9e1ddf121901-0', usage_metadata={'input_tokens': 128, 'output_tokens': 21, 'total_tokens': 149, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "agent = make_agent(model, [add, multiply])\n",
    "\n",
    "for chunk in agent.stream({\"messages\": [(\"user\", \"what's (3 + 5) * 12\")]}):\n",
    "    print(chunk)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09689fde-7a54-4725-859f-b9e7d2725434",
   "metadata": {},
   "source": [
    "Now, we can implement our multi-agent system with the multiplication and addition expert agents. This time we'll give them the tools for doing math, as well as our special handoff tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af9e540a-e847-4ee3-b896-2b4dd93ecb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "addition_expert = make_agent(\n",
    "    model, \n",
    "    [add, make_handoff_tool(agent_name=\"multiplication_expert\")], \n",
    "    system_prompt=\"You are an addition expert, you can ask the multiplication expert for help with multiplication.\"\n",
    ")\n",
    "multiplication_expert = make_agent(\n",
    "    model,\n",
    "    [multiply, make_handoff_tool(agent_name=\"addition_expert\")],\n",
    "    system_prompt=\"You are a multiplication expert, you can ask an addition expert for help with addition.\"\n",
    ")\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"addition_expert\", addition_expert)\n",
    "builder.add_node(\"multiplication_expert\", multiplication_expert)\n",
    "builder.add_edge(START, \"addition_expert\")\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039ff31e-6559-437b-a500-f739b29c003b",
   "metadata": {},
   "source": [
    "Let's run the graph with the same multi-step calculation input as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4ccd402-a90d-4c94-906d-6d364c274192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('addition_expert:0866e746-7feb-f03e-a7d4-a9e1e822acc7',), {'call_model': {'messages': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_IYbgrymbs1OeFeIpqpVEMcyd', 'function': {'arguments': '{\"a\":3,\"b\":5}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 91, 'total_tokens': 108, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_9faba9f038', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-46d8e30e-4f5d-4936-8e58-3b603d007db9-0', tool_calls=[{'name': 'add', 'args': {'a': 3, 'b': 5}, 'id': 'call_IYbgrymbs1OeFeIpqpVEMcyd', 'type': 'tool_call'}], usage_metadata={'input_tokens': 91, 'output_tokens': 17, 'total_tokens': 108, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}})\n",
      "\n",
      "\n",
      "(('addition_expert:0866e746-7feb-f03e-a7d4-a9e1e822acc7',), {'call_tools': {'messages': ToolMessage(content='8', name='add', id='901c6474-17cf-4ca4-9d96-4a5a119fdb0b', tool_call_id='call_IYbgrymbs1OeFeIpqpVEMcyd')}})\n",
      "\n",
      "\n",
      "(('addition_expert:0866e746-7feb-f03e-a7d4-a9e1e822acc7',), {'call_model': {'messages': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_wgWAsyh5csKWc0YG3g9sJks5', 'function': {'arguments': '{}', 'name': 'transfer_to_multiplication_expert'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 116, 'total_tokens': 131, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a79d8dac1f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-85d6b7b0-c76e-4dfe-ad12-59d9d6dd559c-0', tool_calls=[{'name': 'transfer_to_multiplication_expert', 'args': {}, 'id': 'call_wgWAsyh5csKWc0YG3g9sJks5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 116, 'output_tokens': 15, 'total_tokens': 131, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}})\n",
      "\n",
      "\n",
      "((), {'addition_expert': {'messages': [HumanMessage(content=\"what's (3 + 5) * 12\", additional_kwargs={}, response_metadata={}, id='49e2341d-8965-41fe-9f23-dddbdb9f9efd'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_IYbgrymbs1OeFeIpqpVEMcyd', 'function': {'arguments': '{\"a\":3,\"b\":5}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 91, 'total_tokens': 108, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_9faba9f038', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-46d8e30e-4f5d-4936-8e58-3b603d007db9-0', tool_calls=[{'name': 'add', 'args': {'a': 3, 'b': 5}, 'id': 'call_IYbgrymbs1OeFeIpqpVEMcyd', 'type': 'tool_call'}], usage_metadata={'input_tokens': 91, 'output_tokens': 17, 'total_tokens': 108, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='8', name='add', id='901c6474-17cf-4ca4-9d96-4a5a119fdb0b', tool_call_id='call_IYbgrymbs1OeFeIpqpVEMcyd'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_wgWAsyh5csKWc0YG3g9sJks5', 'function': {'arguments': '{}', 'name': 'transfer_to_multiplication_expert'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 116, 'total_tokens': 131, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a79d8dac1f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-85d6b7b0-c76e-4dfe-ad12-59d9d6dd559c-0', tool_calls=[{'name': 'transfer_to_multiplication_expert', 'args': {}, 'id': 'call_wgWAsyh5csKWc0YG3g9sJks5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 116, 'output_tokens': 15, 'total_tokens': 131, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), {'role': 'tool', 'content': 'Successfully transferred to multiplication_expert', 'name': 'transfer_to_multiplication_expert', 'tool_call_id': 'call_wgWAsyh5csKWc0YG3g9sJks5'}]}})\n",
      "\n",
      "\n",
      "(('multiplication_expert:4b9578ac-4ae1-15d0-c2f9-03e0d769cb02',), {'call_model': {'messages': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_lQy42iQUbWI1ySqVFtkdVniM', 'function': {'arguments': '{\"a\":8,\"b\":12}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 151, 'total_tokens': 168, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a79d8dac1f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-bd850102-9f83-4bbb-822c-39b0dd460cdc-0', tool_calls=[{'name': 'multiply', 'args': {'a': 8, 'b': 12}, 'id': 'call_lQy42iQUbWI1ySqVFtkdVniM', 'type': 'tool_call'}], usage_metadata={'input_tokens': 151, 'output_tokens': 17, 'total_tokens': 168, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}})\n",
      "\n",
      "\n",
      "(('multiplication_expert:4b9578ac-4ae1-15d0-c2f9-03e0d769cb02',), {'call_tools': {'messages': ToolMessage(content='96', name='multiply', id='6316bb0b-8221-489b-8d86-1fbdc743f5c6', tool_call_id='call_lQy42iQUbWI1ySqVFtkdVniM')}})\n",
      "\n",
      "\n",
      "(('multiplication_expert:4b9578ac-4ae1-15d0-c2f9-03e0d769cb02',), {'call_model': {'messages': AIMessage(content='The result of (3 + 5) * 12 is 96.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 176, 'total_tokens': 193, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a79d8dac1f', 'finish_reason': 'stop', 'logprobs': None}, id='run-2dd3a52f-f1c8-4886-8dd2-acfefeed438f-0', usage_metadata={'input_tokens': 176, 'output_tokens': 17, 'total_tokens': 193, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}})\n",
      "\n",
      "\n",
      "((), {'multiplication_expert': {'messages': [HumanMessage(content=\"what's (3 + 5) * 12\", additional_kwargs={}, response_metadata={}, id='49e2341d-8965-41fe-9f23-dddbdb9f9efd'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_IYbgrymbs1OeFeIpqpVEMcyd', 'function': {'arguments': '{\"a\":3,\"b\":5}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 91, 'total_tokens': 108, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_9faba9f038', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-46d8e30e-4f5d-4936-8e58-3b603d007db9-0', tool_calls=[{'name': 'add', 'args': {'a': 3, 'b': 5}, 'id': 'call_IYbgrymbs1OeFeIpqpVEMcyd', 'type': 'tool_call'}], usage_metadata={'input_tokens': 91, 'output_tokens': 17, 'total_tokens': 108, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='8', name='add', id='901c6474-17cf-4ca4-9d96-4a5a119fdb0b', tool_call_id='call_IYbgrymbs1OeFeIpqpVEMcyd'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_wgWAsyh5csKWc0YG3g9sJks5', 'function': {'arguments': '{}', 'name': 'transfer_to_multiplication_expert'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 116, 'total_tokens': 131, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a79d8dac1f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-85d6b7b0-c76e-4dfe-ad12-59d9d6dd559c-0', tool_calls=[{'name': 'transfer_to_multiplication_expert', 'args': {}, 'id': 'call_wgWAsyh5csKWc0YG3g9sJks5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 116, 'output_tokens': 15, 'total_tokens': 131, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Successfully transferred to multiplication_expert', name='transfer_to_multiplication_expert', id='7efbd25b-6055-46e5-99b9-b7c99e46f4ce', tool_call_id='call_wgWAsyh5csKWc0YG3g9sJks5'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_lQy42iQUbWI1ySqVFtkdVniM', 'function': {'arguments': '{\"a\":8,\"b\":12}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 151, 'total_tokens': 168, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a79d8dac1f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-bd850102-9f83-4bbb-822c-39b0dd460cdc-0', tool_calls=[{'name': 'multiply', 'args': {'a': 8, 'b': 12}, 'id': 'call_lQy42iQUbWI1ySqVFtkdVniM', 'type': 'tool_call'}], usage_metadata={'input_tokens': 151, 'output_tokens': 17, 'total_tokens': 168, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='96', name='multiply', id='6316bb0b-8221-489b-8d86-1fbdc743f5c6', tool_call_id='call_lQy42iQUbWI1ySqVFtkdVniM'), AIMessage(content='The result of (3 + 5) * 12 is 96.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 176, 'total_tokens': 193, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a79d8dac1f', 'finish_reason': 'stop', 'logprobs': None}, id='run-2dd3a52f-f1c8-4886-8dd2-acfefeed438f-0', usage_metadata={'input_tokens': 176, 'output_tokens': 17, 'total_tokens': 193, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}})\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"what's (3 + 5) * 12\"\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    subgraphs=True\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd98eeae-5abd-45d9-aae7-4e714b7999dc",
   "metadata": {},
   "source": [
    "We can see that after the addition expert is done with the first part of the calculation (after calling the `add` tool), it decides to hand off to the multiplication expert, which computes the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102b116d-62f1-4570-afba-be9a96ce721f",
   "metadata": {},
   "source": [
    "## Using with a prebuilt ReAct agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46194ad-768d-4f29-85ce-91869a220107",
   "metadata": {},
   "source": [
    "If you don't need additional customization, you can use the prebuilt [`create_react_agent`][langgraph.prebuilt.chat_agent_executor.create_react_agent]. It has a built-in support for handoff tools via the prebuilt [`ToolNode`][langgraph.prebuilt.tool_node.ToolNode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe91541c-4c6f-42ef-858a-336bbbb96728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "addition_expert_tools = [add, make_handoff_tool(agent_name=\"multiplication_expert\")]\n",
    "addition_expert = create_react_agent(\n",
    "    # NOTE: we need to explicitly call .bind_tools to disable parallel tool calls\n",
    "    model.bind_tools(addition_expert_tools, parallel_tool_calls=False),\n",
    "    addition_expert_tools, \n",
    "    state_modifier=\"You are an addition expert, you can ask the multiplication expert for help with multiplication.\"\n",
    ")\n",
    "\n",
    "multiplication_expert_tools = [multiply, make_handoff_tool(agent_name=\"addition_expert\")]\n",
    "multiplication_expert = create_react_agent(\n",
    "    model.bind_tools(multiplication_expert_tools, parallel_tool_calls=False),\n",
    "    multiplication_expert_tools,\n",
    "    state_modifier=\"You are a multiplication expert, you can ask an addition expert for help with addition.\"\n",
    ")\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"addition_expert\", addition_expert)\n",
    "builder.add_node(\"multiplication_expert\", multiplication_expert)\n",
    "builder.add_edge(START, \"addition_expert\")\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762fbd94-0e54-45cd-84fb-05a241bae679",
   "metadata": {},
   "source": [
    "We can now verify that the prebuilt ReAct agent works exactly the same as the custom agent above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "637d188c-e0d0-4c05-bb41-f007b4e17fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('addition_expert:a4f5beef-ce3d-c7d9-41b6-5569935290cb',), {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_4nJwNbHe6xShqgjXxIYU45jb', 'function': {'arguments': '{\"a\":3,\"b\":5}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 91, 'total_tokens': 108, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a79d8dac1f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-8b23dec7-d22c-44fc-a9c1-e224690e9043-0', tool_calls=[{'name': 'add', 'args': {'a': 3, 'b': 5}, 'id': 'call_4nJwNbHe6xShqgjXxIYU45jb', 'type': 'tool_call'}], usage_metadata={'input_tokens': 91, 'output_tokens': 17, 'total_tokens': 108, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}})\n",
      "\n",
      "\n",
      "(('addition_expert:a4f5beef-ce3d-c7d9-41b6-5569935290cb',), {'tools': {'messages': [ToolMessage(content='8', name='add', id='979cccef-f9d8-4d64-b82a-6f7f188b8458', tool_call_id='call_4nJwNbHe6xShqgjXxIYU45jb')]}})\n",
      "\n",
      "\n",
      "(('addition_expert:a4f5beef-ce3d-c7d9-41b6-5569935290cb',), {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_XJ7KrZib1CmEOezVlinogel7', 'function': {'arguments': '{}', 'name': 'transfer_to_multiplication_expert'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 116, 'total_tokens': 131, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a79d8dac1f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-13c42130-22b6-4bf4-a931-294151ae353c-0', tool_calls=[{'name': 'transfer_to_multiplication_expert', 'args': {}, 'id': 'call_XJ7KrZib1CmEOezVlinogel7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 116, 'output_tokens': 15, 'total_tokens': 131, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}})\n",
      "\n",
      "\n",
      "((), {'addition_expert': {'messages': [HumanMessage(content=\"what's (3 + 5) * 12\", additional_kwargs={}, response_metadata={}, id='2fcf1db2-15d9-48ef-8848-aa3094ce907c'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_4nJwNbHe6xShqgjXxIYU45jb', 'function': {'arguments': '{\"a\":3,\"b\":5}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 91, 'total_tokens': 108, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a79d8dac1f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-8b23dec7-d22c-44fc-a9c1-e224690e9043-0', tool_calls=[{'name': 'add', 'args': {'a': 3, 'b': 5}, 'id': 'call_4nJwNbHe6xShqgjXxIYU45jb', 'type': 'tool_call'}], usage_metadata={'input_tokens': 91, 'output_tokens': 17, 'total_tokens': 108, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='8', name='add', id='979cccef-f9d8-4d64-b82a-6f7f188b8458', tool_call_id='call_4nJwNbHe6xShqgjXxIYU45jb'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_XJ7KrZib1CmEOezVlinogel7', 'function': {'arguments': '{}', 'name': 'transfer_to_multiplication_expert'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 116, 'total_tokens': 131, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a79d8dac1f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-13c42130-22b6-4bf4-a931-294151ae353c-0', tool_calls=[{'name': 'transfer_to_multiplication_expert', 'args': {}, 'id': 'call_XJ7KrZib1CmEOezVlinogel7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 116, 'output_tokens': 15, 'total_tokens': 131, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), {'role': 'tool', 'content': 'Successfully transferred to multiplication_expert', 'name': 'transfer_to_multiplication_expert', 'tool_call_id': 'call_XJ7KrZib1CmEOezVlinogel7'}]}})\n",
      "\n",
      "\n",
      "(('multiplication_expert:906e28ac-15b0-bacf-3b70-218644efcb11',), {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_D1xRc6sKKlVhpr608APuweUa', 'function': {'arguments': '{\"a\":8,\"b\":12}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 151, 'total_tokens': 168, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a79d8dac1f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-0c908e67-78e9-48e5-b877-9477294ad679-0', tool_calls=[{'name': 'multiply', 'args': {'a': 8, 'b': 12}, 'id': 'call_D1xRc6sKKlVhpr608APuweUa', 'type': 'tool_call'}], usage_metadata={'input_tokens': 151, 'output_tokens': 17, 'total_tokens': 168, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}})\n",
      "\n",
      "\n",
      "(('multiplication_expert:906e28ac-15b0-bacf-3b70-218644efcb11',), {'tools': {'messages': [ToolMessage(content='96', name='multiply', id='860c0a07-8577-4b40-93e2-db111409dc3d', tool_call_id='call_D1xRc6sKKlVhpr608APuweUa')]}})\n",
      "\n",
      "\n",
      "(('multiplication_expert:906e28ac-15b0-bacf-3b70-218644efcb11',), {'agent': {'messages': [AIMessage(content='The result of \\\\((3 + 5) * 12\\\\) is 96.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 176, 'total_tokens': 196, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a79d8dac1f', 'finish_reason': 'stop', 'logprobs': None}, id='run-3c7a732d-59f1-419f-8ec2-bdb90b69ad3f-0', usage_metadata={'input_tokens': 176, 'output_tokens': 20, 'total_tokens': 196, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}})\n",
      "\n",
      "\n",
      "((), {'multiplication_expert': {'messages': [HumanMessage(content=\"what's (3 + 5) * 12\", additional_kwargs={}, response_metadata={}, id='2fcf1db2-15d9-48ef-8848-aa3094ce907c'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_4nJwNbHe6xShqgjXxIYU45jb', 'function': {'arguments': '{\"a\":3,\"b\":5}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 91, 'total_tokens': 108, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a79d8dac1f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-8b23dec7-d22c-44fc-a9c1-e224690e9043-0', tool_calls=[{'name': 'add', 'args': {'a': 3, 'b': 5}, 'id': 'call_4nJwNbHe6xShqgjXxIYU45jb', 'type': 'tool_call'}], usage_metadata={'input_tokens': 91, 'output_tokens': 17, 'total_tokens': 108, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='8', name='add', id='979cccef-f9d8-4d64-b82a-6f7f188b8458', tool_call_id='call_4nJwNbHe6xShqgjXxIYU45jb'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_XJ7KrZib1CmEOezVlinogel7', 'function': {'arguments': '{}', 'name': 'transfer_to_multiplication_expert'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 116, 'total_tokens': 131, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a79d8dac1f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-13c42130-22b6-4bf4-a931-294151ae353c-0', tool_calls=[{'name': 'transfer_to_multiplication_expert', 'args': {}, 'id': 'call_XJ7KrZib1CmEOezVlinogel7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 116, 'output_tokens': 15, 'total_tokens': 131, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Successfully transferred to multiplication_expert', name='transfer_to_multiplication_expert', id='c3288f35-e873-4124-9abe-3c7b19511c41', tool_call_id='call_XJ7KrZib1CmEOezVlinogel7'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_D1xRc6sKKlVhpr608APuweUa', 'function': {'arguments': '{\"a\":8,\"b\":12}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 151, 'total_tokens': 168, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a79d8dac1f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-0c908e67-78e9-48e5-b877-9477294ad679-0', tool_calls=[{'name': 'multiply', 'args': {'a': 8, 'b': 12}, 'id': 'call_D1xRc6sKKlVhpr608APuweUa', 'type': 'tool_call'}], usage_metadata={'input_tokens': 151, 'output_tokens': 17, 'total_tokens': 168, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='96', name='multiply', id='860c0a07-8577-4b40-93e2-db111409dc3d', tool_call_id='call_D1xRc6sKKlVhpr608APuweUa'), AIMessage(content='The result of \\\\((3 + 5) * 12\\\\) is 96.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 176, 'total_tokens': 196, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a79d8dac1f', 'finish_reason': 'stop', 'logprobs': None}, id='run-3c7a732d-59f1-419f-8ec2-bdb90b69ad3f-0', usage_metadata={'input_tokens': 176, 'output_tokens': 20, 'total_tokens': 196, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}})\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"what's (3 + 5) * 12\"\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    subgraphs=True\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
